{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train),(X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], 784)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], 784)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "scaler.fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(784,)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "modelpath = \"./model_optimization/mnist_MLP.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1493/1500 [============================>.] - ETA: 0s - loss: 1.8012 - accuracy: 0.5020\n",
      "Epoch 1: val_loss improved from inf to 1.36022, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 763us/step - loss: 1.7993 - accuracy: 0.5030 - val_loss: 1.3602 - val_accuracy: 0.7429\n",
      "Epoch 2/100\n",
      "1460/1500 [============================>.] - ETA: 0s - loss: 1.1814 - accuracy: 0.7652\n",
      "Epoch 2: val_loss improved from 1.36022 to 0.98980, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 688us/step - loss: 1.1777 - accuracy: 0.7656 - val_loss: 0.9898 - val_accuracy: 0.8083\n",
      "Epoch 3/100\n",
      "1489/1500 [============================>.] - ETA: 0s - loss: 0.9313 - accuracy: 0.8078\n",
      "Epoch 3: val_loss improved from 0.98980 to 0.81707, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 0.9308 - accuracy: 0.8078 - val_loss: 0.8171 - val_accuracy: 0.8381\n",
      "Epoch 4/100\n",
      "1445/1500 [===========================>..] - ETA: 0s - loss: 0.8046 - accuracy: 0.8275\n",
      "Epoch 4: val_loss improved from 0.81707 to 0.71794, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 0.8023 - accuracy: 0.8283 - val_loss: 0.7179 - val_accuracy: 0.8517\n",
      "Epoch 5/100\n",
      "1472/1500 [============================>.] - ETA: 0s - loss: 0.7233 - accuracy: 0.8398\n",
      "Epoch 5: val_loss improved from 0.71794 to 0.65321, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 0.7230 - accuracy: 0.8397 - val_loss: 0.6532 - val_accuracy: 0.8589\n",
      "Epoch 6/100\n",
      "1441/1500 [===========================>..] - ETA: 0s - loss: 0.6683 - accuracy: 0.8480\n",
      "Epoch 6: val_loss improved from 0.65321 to 0.60731, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 688us/step - loss: 0.6685 - accuracy: 0.8474 - val_loss: 0.6073 - val_accuracy: 0.8652\n",
      "Epoch 7/100\n",
      "1440/1500 [===========================>..] - ETA: 0s - loss: 0.6288 - accuracy: 0.8534\n",
      "Epoch 7: val_loss improved from 0.60731 to 0.57290, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 711us/step - loss: 0.6285 - accuracy: 0.8535 - val_loss: 0.5729 - val_accuracy: 0.8710\n",
      "Epoch 8/100\n",
      "1460/1500 [============================>.] - ETA: 0s - loss: 0.5977 - accuracy: 0.8581\n",
      "Epoch 8: val_loss improved from 0.57290 to 0.54610, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 722us/step - loss: 0.5977 - accuracy: 0.8583 - val_loss: 0.5461 - val_accuracy: 0.8733\n",
      "Epoch 9/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.5731 - accuracy: 0.8619\n",
      "Epoch 9: val_loss improved from 0.54610 to 0.52458, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 0.5729 - accuracy: 0.8620 - val_loss: 0.5246 - val_accuracy: 0.8756\n",
      "Epoch 10/100\n",
      "1416/1500 [===========================>..] - ETA: 0s - loss: 0.5527 - accuracy: 0.8645\n",
      "Epoch 10: val_loss improved from 0.52458 to 0.50667, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 0.5527 - accuracy: 0.8645 - val_loss: 0.5067 - val_accuracy: 0.8792\n",
      "Epoch 11/100\n",
      "1430/1500 [===========================>..] - ETA: 0s - loss: 0.5369 - accuracy: 0.8675\n",
      "Epoch 11: val_loss improved from 0.50667 to 0.49180, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 705us/step - loss: 0.5357 - accuracy: 0.8676 - val_loss: 0.4918 - val_accuracy: 0.8810\n",
      "Epoch 12/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 0.5215 - accuracy: 0.8700\n",
      "Epoch 12: val_loss improved from 0.49180 to 0.47888, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 724us/step - loss: 0.5212 - accuracy: 0.8701 - val_loss: 0.4789 - val_accuracy: 0.8837\n",
      "Epoch 13/100\n",
      "1426/1500 [===========================>..] - ETA: 0s - loss: 0.5076 - accuracy: 0.8724\n",
      "Epoch 13: val_loss improved from 0.47888 to 0.46774, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 701us/step - loss: 0.5085 - accuracy: 0.8722 - val_loss: 0.4677 - val_accuracy: 0.8848\n",
      "Epoch 14/100\n",
      "1441/1500 [===========================>..] - ETA: 0s - loss: 0.4980 - accuracy: 0.8737\n",
      "Epoch 14: val_loss improved from 0.46774 to 0.45785, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 694us/step - loss: 0.4975 - accuracy: 0.8740 - val_loss: 0.4578 - val_accuracy: 0.8870\n",
      "Epoch 15/100\n",
      "1425/1500 [===========================>..] - ETA: 0s - loss: 0.4891 - accuracy: 0.8750\n",
      "Epoch 15: val_loss improved from 0.45785 to 0.44935, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 698us/step - loss: 0.4877 - accuracy: 0.8756 - val_loss: 0.4494 - val_accuracy: 0.8874\n",
      "Epoch 16/100\n",
      "1456/1500 [============================>.] - ETA: 0s - loss: 0.4790 - accuracy: 0.8766\n",
      "Epoch 16: val_loss improved from 0.44935 to 0.44148, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 690us/step - loss: 0.4790 - accuracy: 0.8765 - val_loss: 0.4415 - val_accuracy: 0.8892\n",
      "Epoch 17/100\n",
      "1420/1500 [===========================>..] - ETA: 0s - loss: 0.4706 - accuracy: 0.8783\n",
      "Epoch 17: val_loss improved from 0.44148 to 0.43448, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 704us/step - loss: 0.4711 - accuracy: 0.8780 - val_loss: 0.4345 - val_accuracy: 0.8901\n",
      "Epoch 18/100\n",
      "1440/1500 [===========================>..] - ETA: 0s - loss: 0.4642 - accuracy: 0.8793\n",
      "Epoch 18: val_loss improved from 0.43448 to 0.42820, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 697us/step - loss: 0.4639 - accuracy: 0.8796 - val_loss: 0.4282 - val_accuracy: 0.8913\n",
      "Epoch 19/100\n",
      "1420/1500 [===========================>..] - ETA: 0s - loss: 0.4578 - accuracy: 0.8807\n",
      "Epoch 19: val_loss improved from 0.42820 to 0.42247, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 699us/step - loss: 0.4574 - accuracy: 0.8809 - val_loss: 0.4225 - val_accuracy: 0.8919\n",
      "Epoch 20/100\n",
      "1431/1500 [===========================>..] - ETA: 0s - loss: 0.4522 - accuracy: 0.8821\n",
      "Epoch 20: val_loss improved from 0.42247 to 0.41719, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 700us/step - loss: 0.4514 - accuracy: 0.8821 - val_loss: 0.4172 - val_accuracy: 0.8928\n",
      "Epoch 21/100\n",
      "1462/1500 [============================>.] - ETA: 0s - loss: 0.4459 - accuracy: 0.8831\n",
      "Epoch 21: val_loss improved from 0.41719 to 0.41238, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 690us/step - loss: 0.4459 - accuracy: 0.8829 - val_loss: 0.4124 - val_accuracy: 0.8938\n",
      "Epoch 22/100\n",
      "1423/1500 [===========================>..] - ETA: 0s - loss: 0.4408 - accuracy: 0.8840\n",
      "Epoch 22: val_loss improved from 0.41238 to 0.40786, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 697us/step - loss: 0.4408 - accuracy: 0.8838 - val_loss: 0.4079 - val_accuracy: 0.8942\n",
      "Epoch 23/100\n",
      "1420/1500 [===========================>..] - ETA: 0s - loss: 0.4362 - accuracy: 0.8850\n",
      "Epoch 23: val_loss improved from 0.40786 to 0.40381, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 0.4360 - accuracy: 0.8851 - val_loss: 0.4038 - val_accuracy: 0.8950\n",
      "Epoch 24/100\n",
      "1423/1500 [===========================>..] - ETA: 0s - loss: 0.4321 - accuracy: 0.8852\n",
      "Epoch 24: val_loss improved from 0.40381 to 0.39982, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 0.4316 - accuracy: 0.8856 - val_loss: 0.3998 - val_accuracy: 0.8959\n",
      "Epoch 25/100\n",
      "1425/1500 [===========================>..] - ETA: 0s - loss: 0.4283 - accuracy: 0.8858\n",
      "Epoch 25: val_loss improved from 0.39982 to 0.39629, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 705us/step - loss: 0.4275 - accuracy: 0.8863 - val_loss: 0.3963 - val_accuracy: 0.8967\n",
      "Epoch 26/100\n",
      "1436/1500 [===========================>..] - ETA: 0s - loss: 0.4228 - accuracy: 0.8877\n",
      "Epoch 26: val_loss improved from 0.39629 to 0.39285, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 693us/step - loss: 0.4236 - accuracy: 0.8872 - val_loss: 0.3929 - val_accuracy: 0.8983\n",
      "Epoch 27/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8882\n",
      "Epoch 27: val_loss improved from 0.39285 to 0.38971, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 709us/step - loss: 0.4200 - accuracy: 0.8880 - val_loss: 0.3897 - val_accuracy: 0.8986\n",
      "Epoch 28/100\n",
      "1428/1500 [===========================>..] - ETA: 0s - loss: 0.4165 - accuracy: 0.8888\n",
      "Epoch 28: val_loss improved from 0.38971 to 0.38670, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 701us/step - loss: 0.4166 - accuracy: 0.8886 - val_loss: 0.3867 - val_accuracy: 0.8991\n",
      "Epoch 29/100\n",
      "1453/1500 [============================>.] - ETA: 0s - loss: 0.4144 - accuracy: 0.8889\n",
      "Epoch 29: val_loss improved from 0.38670 to 0.38395, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 691us/step - loss: 0.4133 - accuracy: 0.8893 - val_loss: 0.3840 - val_accuracy: 0.8992\n",
      "Epoch 30/100\n",
      "1434/1500 [===========================>..] - ETA: 0s - loss: 0.4113 - accuracy: 0.8890\n",
      "Epoch 30: val_loss improved from 0.38395 to 0.38126, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 700us/step - loss: 0.4103 - accuracy: 0.8894 - val_loss: 0.3813 - val_accuracy: 0.8997\n",
      "Epoch 31/100\n",
      "1436/1500 [===========================>..] - ETA: 0s - loss: 0.4071 - accuracy: 0.8904\n",
      "Epoch 31: val_loss improved from 0.38126 to 0.37870, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 696us/step - loss: 0.4073 - accuracy: 0.8902 - val_loss: 0.3787 - val_accuracy: 0.9004\n",
      "Epoch 32/100\n",
      "1425/1500 [===========================>..] - ETA: 0s - loss: 0.4052 - accuracy: 0.8911\n",
      "Epoch 32: val_loss improved from 0.37870 to 0.37639, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 0.4046 - accuracy: 0.8911 - val_loss: 0.3764 - val_accuracy: 0.9003\n",
      "Epoch 33/100\n",
      "1429/1500 [===========================>..] - ETA: 0s - loss: 0.4026 - accuracy: 0.8908\n",
      "Epoch 33: val_loss improved from 0.37639 to 0.37404, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 697us/step - loss: 0.4019 - accuracy: 0.8910 - val_loss: 0.3740 - val_accuracy: 0.9013\n",
      "Epoch 34/100\n",
      "1415/1500 [===========================>..] - ETA: 0s - loss: 0.4003 - accuracy: 0.8910\n",
      "Epoch 34: val_loss improved from 0.37404 to 0.37187, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 0.3994 - accuracy: 0.8916 - val_loss: 0.3719 - val_accuracy: 0.9015\n",
      "Epoch 35/100\n",
      "1414/1500 [===========================>..] - ETA: 0s - loss: 0.3966 - accuracy: 0.8920\n",
      "Epoch 35: val_loss improved from 0.37187 to 0.36989, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 702us/step - loss: 0.3970 - accuracy: 0.8923 - val_loss: 0.3699 - val_accuracy: 0.9013\n",
      "Epoch 36/100\n",
      "1455/1500 [============================>.] - ETA: 0s - loss: 0.3956 - accuracy: 0.8921\n",
      "Epoch 36: val_loss improved from 0.36989 to 0.36795, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 0.3947 - accuracy: 0.8927 - val_loss: 0.3679 - val_accuracy: 0.9018\n",
      "Epoch 37/100\n",
      "1488/1500 [============================>.] - ETA: 0s - loss: 0.3919 - accuracy: 0.8932\n",
      "Epoch 37: val_loss improved from 0.36795 to 0.36594, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 712us/step - loss: 0.3925 - accuracy: 0.8930 - val_loss: 0.3659 - val_accuracy: 0.9028\n",
      "Epoch 38/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.3904 - accuracy: 0.8936\n",
      "Epoch 38: val_loss improved from 0.36594 to 0.36421, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 716us/step - loss: 0.3904 - accuracy: 0.8936 - val_loss: 0.3642 - val_accuracy: 0.9028\n",
      "Epoch 39/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.3887 - accuracy: 0.8942\n",
      "Epoch 39: val_loss improved from 0.36421 to 0.36251, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 718us/step - loss: 0.3884 - accuracy: 0.8942 - val_loss: 0.3625 - val_accuracy: 0.9031\n",
      "Epoch 40/100\n",
      "1425/1500 [===========================>..] - ETA: 0s - loss: 0.3873 - accuracy: 0.8939\n",
      "Epoch 40: val_loss improved from 0.36251 to 0.36082, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 0.3865 - accuracy: 0.8944 - val_loss: 0.3608 - val_accuracy: 0.9035\n",
      "Epoch 41/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 0.3841 - accuracy: 0.8952\n",
      "Epoch 41: val_loss improved from 0.36082 to 0.35928, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 791us/step - loss: 0.3846 - accuracy: 0.8951 - val_loss: 0.3593 - val_accuracy: 0.9036\n",
      "Epoch 42/100\n",
      "1438/1500 [===========================>..] - ETA: 0s - loss: 0.3824 - accuracy: 0.8953\n",
      "Epoch 42: val_loss improved from 0.35928 to 0.35770, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 775us/step - loss: 0.3828 - accuracy: 0.8953 - val_loss: 0.3577 - val_accuracy: 0.9045\n",
      "Epoch 43/100\n",
      "1477/1500 [============================>.] - ETA: 0s - loss: 0.3814 - accuracy: 0.8959\n",
      "Epoch 43: val_loss improved from 0.35770 to 0.35627, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 776us/step - loss: 0.3811 - accuracy: 0.8961 - val_loss: 0.3563 - val_accuracy: 0.9040\n",
      "Epoch 44/100\n",
      "1441/1500 [===========================>..] - ETA: 0s - loss: 0.3789 - accuracy: 0.8961\n",
      "Epoch 44: val_loss improved from 0.35627 to 0.35483, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 860us/step - loss: 0.3794 - accuracy: 0.8960 - val_loss: 0.3548 - val_accuracy: 0.9051\n",
      "Epoch 45/100\n",
      "1462/1500 [============================>.] - ETA: 0s - loss: 0.3779 - accuracy: 0.8964\n",
      "Epoch 45: val_loss improved from 0.35483 to 0.35341, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 816us/step - loss: 0.3778 - accuracy: 0.8964 - val_loss: 0.3534 - val_accuracy: 0.9059\n",
      "Epoch 46/100\n",
      "1442/1500 [===========================>..] - ETA: 0s - loss: 0.3770 - accuracy: 0.8962\n",
      "Epoch 46: val_loss improved from 0.35341 to 0.35212, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 721us/step - loss: 0.3762 - accuracy: 0.8966 - val_loss: 0.3521 - val_accuracy: 0.9058\n",
      "Epoch 47/100\n",
      "1422/1500 [===========================>..] - ETA: 0s - loss: 0.3747 - accuracy: 0.8970\n",
      "Epoch 47: val_loss improved from 0.35212 to 0.35080, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 0.3747 - accuracy: 0.8970 - val_loss: 0.3508 - val_accuracy: 0.9062\n",
      "Epoch 48/100\n",
      "1432/1500 [===========================>..] - ETA: 0s - loss: 0.3737 - accuracy: 0.8973\n",
      "Epoch 48: val_loss improved from 0.35080 to 0.34957, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 0.3733 - accuracy: 0.8975 - val_loss: 0.3496 - val_accuracy: 0.9068\n",
      "Epoch 49/100\n",
      "1463/1500 [============================>.] - ETA: 0s - loss: 0.3712 - accuracy: 0.8978\n",
      "Epoch 49: val_loss improved from 0.34957 to 0.34838, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 0.3718 - accuracy: 0.8976 - val_loss: 0.3484 - val_accuracy: 0.9068\n",
      "Epoch 50/100\n",
      "1421/1500 [===========================>..] - ETA: 0s - loss: 0.3722 - accuracy: 0.8978\n",
      "Epoch 50: val_loss improved from 0.34838 to 0.34726, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 0.3705 - accuracy: 0.8981 - val_loss: 0.3473 - val_accuracy: 0.9068\n",
      "Epoch 51/100\n",
      "1442/1500 [===========================>..] - ETA: 0s - loss: 0.3706 - accuracy: 0.8977\n",
      "Epoch 51: val_loss improved from 0.34726 to 0.34610, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 691us/step - loss: 0.3691 - accuracy: 0.8983 - val_loss: 0.3461 - val_accuracy: 0.9073\n",
      "Epoch 52/100\n",
      "1433/1500 [===========================>..] - ETA: 0s - loss: 0.3656 - accuracy: 0.8986\n",
      "Epoch 52: val_loss improved from 0.34610 to 0.34502, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 0.3678 - accuracy: 0.8984 - val_loss: 0.3450 - val_accuracy: 0.9072\n",
      "Epoch 53/100\n",
      "1480/1500 [============================>.] - ETA: 0s - loss: 0.3666 - accuracy: 0.8989\n",
      "Epoch 53: val_loss improved from 0.34502 to 0.34401, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 714us/step - loss: 0.3666 - accuracy: 0.8990 - val_loss: 0.3440 - val_accuracy: 0.9078\n",
      "Epoch 54/100\n",
      "1437/1500 [===========================>..] - ETA: 0s - loss: 0.3660 - accuracy: 0.8990\n",
      "Epoch 54: val_loss improved from 0.34401 to 0.34294, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 699us/step - loss: 0.3654 - accuracy: 0.8991 - val_loss: 0.3429 - val_accuracy: 0.9080\n",
      "Epoch 55/100\n",
      "1495/1500 [============================>.] - ETA: 0s - loss: 0.3638 - accuracy: 0.8995\n",
      "Epoch 55: val_loss improved from 0.34294 to 0.34197, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 706us/step - loss: 0.3642 - accuracy: 0.8994 - val_loss: 0.3420 - val_accuracy: 0.9080\n",
      "Epoch 56/100\n",
      "1428/1500 [===========================>..] - ETA: 0s - loss: 0.3625 - accuracy: 0.8995\n",
      "Epoch 56: val_loss improved from 0.34197 to 0.34099, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 697us/step - loss: 0.3630 - accuracy: 0.8996 - val_loss: 0.3410 - val_accuracy: 0.9083\n",
      "Epoch 57/100\n",
      "1430/1500 [===========================>..] - ETA: 0s - loss: 0.3617 - accuracy: 0.8996\n",
      "Epoch 57: val_loss improved from 0.34099 to 0.34002, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 697us/step - loss: 0.3619 - accuracy: 0.8998 - val_loss: 0.3400 - val_accuracy: 0.9087\n",
      "Epoch 58/100\n",
      "1475/1500 [============================>.] - ETA: 0s - loss: 0.3610 - accuracy: 0.9005\n",
      "Epoch 58: val_loss improved from 0.34002 to 0.33914, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 678us/step - loss: 0.3608 - accuracy: 0.9003 - val_loss: 0.3391 - val_accuracy: 0.9087\n",
      "Epoch 59/100\n",
      "1460/1500 [============================>.] - ETA: 0s - loss: 0.3585 - accuracy: 0.9008\n",
      "Epoch 59: val_loss improved from 0.33914 to 0.33824, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 684us/step - loss: 0.3598 - accuracy: 0.9005 - val_loss: 0.3382 - val_accuracy: 0.9093\n",
      "Epoch 60/100\n",
      "1416/1500 [===========================>..] - ETA: 0s - loss: 0.3576 - accuracy: 0.9012\n",
      "Epoch 60: val_loss improved from 0.33824 to 0.33743, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 708us/step - loss: 0.3587 - accuracy: 0.9007 - val_loss: 0.3374 - val_accuracy: 0.9099\n",
      "Epoch 61/100\n",
      "1422/1500 [===========================>..] - ETA: 0s - loss: 0.3579 - accuracy: 0.9010\n",
      "Epoch 61: val_loss improved from 0.33743 to 0.33655, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 720us/step - loss: 0.3577 - accuracy: 0.9011 - val_loss: 0.3366 - val_accuracy: 0.9100\n",
      "Epoch 62/100\n",
      "1435/1500 [===========================>..] - ETA: 0s - loss: 0.3573 - accuracy: 0.9011\n",
      "Epoch 62: val_loss improved from 0.33655 to 0.33573, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 780us/step - loss: 0.3567 - accuracy: 0.9011 - val_loss: 0.3357 - val_accuracy: 0.9100\n",
      "Epoch 63/100\n",
      "1476/1500 [============================>.] - ETA: 0s - loss: 0.3553 - accuracy: 0.9015\n",
      "Epoch 63: val_loss improved from 0.33573 to 0.33490, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 768us/step - loss: 0.3558 - accuracy: 0.9014 - val_loss: 0.3349 - val_accuracy: 0.9102\n",
      "Epoch 64/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.3546 - accuracy: 0.9017\n",
      "Epoch 64: val_loss improved from 0.33490 to 0.33408, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 793us/step - loss: 0.3548 - accuracy: 0.9017 - val_loss: 0.3341 - val_accuracy: 0.9105\n",
      "Epoch 65/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.3530 - accuracy: 0.9022\n",
      "Epoch 65: val_loss improved from 0.33408 to 0.33336, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 778us/step - loss: 0.3539 - accuracy: 0.9021 - val_loss: 0.3334 - val_accuracy: 0.9106\n",
      "Epoch 66/100\n",
      "1428/1500 [===========================>..] - ETA: 0s - loss: 0.3536 - accuracy: 0.9020\n",
      "Epoch 66: val_loss improved from 0.33336 to 0.33262, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 744us/step - loss: 0.3530 - accuracy: 0.9022 - val_loss: 0.3326 - val_accuracy: 0.9107\n",
      "Epoch 67/100\n",
      "1496/1500 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.9023\n",
      "Epoch 67: val_loss improved from 0.33262 to 0.33187, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 751us/step - loss: 0.3521 - accuracy: 0.9023 - val_loss: 0.3319 - val_accuracy: 0.9110\n",
      "Epoch 68/100\n",
      "1481/1500 [============================>.] - ETA: 0s - loss: 0.3508 - accuracy: 0.9027\n",
      "Epoch 68: val_loss improved from 0.33187 to 0.33116, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 0.3513 - accuracy: 0.9027 - val_loss: 0.3312 - val_accuracy: 0.9112\n",
      "Epoch 69/100\n",
      "1454/1500 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.9031\n",
      "Epoch 69: val_loss improved from 0.33116 to 0.33045, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 773us/step - loss: 0.3504 - accuracy: 0.9028 - val_loss: 0.3305 - val_accuracy: 0.9113\n",
      "Epoch 70/100\n",
      "1461/1500 [============================>.] - ETA: 0s - loss: 0.3499 - accuracy: 0.9032\n",
      "Epoch 70: val_loss improved from 0.33045 to 0.32979, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 0.3496 - accuracy: 0.9032 - val_loss: 0.3298 - val_accuracy: 0.9114\n",
      "Epoch 71/100\n",
      "1442/1500 [===========================>..] - ETA: 0s - loss: 0.3488 - accuracy: 0.9033\n",
      "Epoch 71: val_loss improved from 0.32979 to 0.32912, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 778us/step - loss: 0.3488 - accuracy: 0.9032 - val_loss: 0.3291 - val_accuracy: 0.9114\n",
      "Epoch 72/100\n",
      "1436/1500 [===========================>..] - ETA: 0s - loss: 0.3480 - accuracy: 0.9035\n",
      "Epoch 72: val_loss improved from 0.32912 to 0.32846, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 792us/step - loss: 0.3480 - accuracy: 0.9034 - val_loss: 0.3285 - val_accuracy: 0.9116\n",
      "Epoch 73/100\n",
      "1461/1500 [============================>.] - ETA: 0s - loss: 0.3471 - accuracy: 0.9034\n",
      "Epoch 73: val_loss improved from 0.32846 to 0.32785, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 768us/step - loss: 0.3472 - accuracy: 0.9034 - val_loss: 0.3278 - val_accuracy: 0.9113\n",
      "Epoch 74/100\n",
      "1476/1500 [============================>.] - ETA: 0s - loss: 0.3452 - accuracy: 0.9043\n",
      "Epoch 74: val_loss improved from 0.32785 to 0.32720, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 763us/step - loss: 0.3465 - accuracy: 0.9038 - val_loss: 0.3272 - val_accuracy: 0.9115\n",
      "Epoch 75/100\n",
      "1432/1500 [===========================>..] - ETA: 0s - loss: 0.3462 - accuracy: 0.9035\n",
      "Epoch 75: val_loss improved from 0.32720 to 0.32658, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 0.3457 - accuracy: 0.9038 - val_loss: 0.3266 - val_accuracy: 0.9115\n",
      "Epoch 76/100\n",
      "1418/1500 [===========================>..] - ETA: 0s - loss: 0.3438 - accuracy: 0.9046\n",
      "Epoch 76: val_loss improved from 0.32658 to 0.32601, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 752us/step - loss: 0.3450 - accuracy: 0.9043 - val_loss: 0.3260 - val_accuracy: 0.9116\n",
      "Epoch 77/100\n",
      "1458/1500 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.9041\n",
      "Epoch 77: val_loss improved from 0.32601 to 0.32542, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 731us/step - loss: 0.3443 - accuracy: 0.9043 - val_loss: 0.3254 - val_accuracy: 0.9121\n",
      "Epoch 78/100\n",
      "1478/1500 [============================>.] - ETA: 0s - loss: 0.3436 - accuracy: 0.9043\n",
      "Epoch 78: val_loss improved from 0.32542 to 0.32483, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 759us/step - loss: 0.3436 - accuracy: 0.9044 - val_loss: 0.3248 - val_accuracy: 0.9121\n",
      "Epoch 79/100\n",
      "1459/1500 [============================>.] - ETA: 0s - loss: 0.3418 - accuracy: 0.9052\n",
      "Epoch 79: val_loss improved from 0.32483 to 0.32428, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 772us/step - loss: 0.3429 - accuracy: 0.9049 - val_loss: 0.3243 - val_accuracy: 0.9119\n",
      "Epoch 80/100\n",
      "1439/1500 [===========================>..] - ETA: 0s - loss: 0.3422 - accuracy: 0.9053\n",
      "Epoch 80: val_loss improved from 0.32428 to 0.32379, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 743us/step - loss: 0.3422 - accuracy: 0.9053 - val_loss: 0.3238 - val_accuracy: 0.9124\n",
      "Epoch 81/100\n",
      "1485/1500 [============================>.] - ETA: 0s - loss: 0.3422 - accuracy: 0.9048\n",
      "Epoch 81: val_loss improved from 0.32379 to 0.32321, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 767us/step - loss: 0.3416 - accuracy: 0.9050 - val_loss: 0.3232 - val_accuracy: 0.9122\n",
      "Epoch 82/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.3407 - accuracy: 0.9053\n",
      "Epoch 82: val_loss improved from 0.32321 to 0.32267, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 853us/step - loss: 0.3409 - accuracy: 0.9052 - val_loss: 0.3227 - val_accuracy: 0.9123\n",
      "Epoch 83/100\n",
      "1492/1500 [============================>.] - ETA: 0s - loss: 0.3409 - accuracy: 0.9053\n",
      "Epoch 83: val_loss improved from 0.32267 to 0.32214, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 790us/step - loss: 0.3403 - accuracy: 0.9055 - val_loss: 0.3221 - val_accuracy: 0.9125\n",
      "Epoch 84/100\n",
      "1494/1500 [============================>.] - ETA: 0s - loss: 0.3397 - accuracy: 0.9060\n",
      "Epoch 84: val_loss improved from 0.32214 to 0.32164, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 756us/step - loss: 0.3397 - accuracy: 0.9058 - val_loss: 0.3216 - val_accuracy: 0.9125\n",
      "Epoch 85/100\n",
      "1439/1500 [===========================>..] - ETA: 0s - loss: 0.3384 - accuracy: 0.9061\n",
      "Epoch 85: val_loss improved from 0.32164 to 0.32111, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 741us/step - loss: 0.3391 - accuracy: 0.9059 - val_loss: 0.3211 - val_accuracy: 0.9126\n",
      "Epoch 86/100\n",
      "1439/1500 [===========================>..] - ETA: 0s - loss: 0.3385 - accuracy: 0.9060\n",
      "Epoch 86: val_loss improved from 0.32111 to 0.32066, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 735us/step - loss: 0.3384 - accuracy: 0.9061 - val_loss: 0.3207 - val_accuracy: 0.9129\n",
      "Epoch 87/100\n",
      "1462/1500 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.9066\n",
      "Epoch 87: val_loss improved from 0.32066 to 0.32014, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 0.3379 - accuracy: 0.9062 - val_loss: 0.3201 - val_accuracy: 0.9129\n",
      "Epoch 88/100\n",
      "1482/1500 [============================>.] - ETA: 0s - loss: 0.3373 - accuracy: 0.9066\n",
      "Epoch 88: val_loss improved from 0.32014 to 0.31968, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 758us/step - loss: 0.3373 - accuracy: 0.9065 - val_loss: 0.3197 - val_accuracy: 0.9130\n",
      "Epoch 89/100\n",
      "1484/1500 [============================>.] - ETA: 0s - loss: 0.3366 - accuracy: 0.9066\n",
      "Epoch 89: val_loss improved from 0.31968 to 0.31921, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 761us/step - loss: 0.3367 - accuracy: 0.9066 - val_loss: 0.3192 - val_accuracy: 0.9130\n",
      "Epoch 90/100\n",
      "1457/1500 [============================>.] - ETA: 0s - loss: 0.3365 - accuracy: 0.9067\n",
      "Epoch 90: val_loss improved from 0.31921 to 0.31878, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 762us/step - loss: 0.3361 - accuracy: 0.9067 - val_loss: 0.3188 - val_accuracy: 0.9131\n",
      "Epoch 91/100\n",
      "1446/1500 [===========================>..] - ETA: 0s - loss: 0.3346 - accuracy: 0.9072\n",
      "Epoch 91: val_loss improved from 0.31878 to 0.31833, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 799us/step - loss: 0.3356 - accuracy: 0.9069 - val_loss: 0.3183 - val_accuracy: 0.9130\n",
      "Epoch 92/100\n",
      "1471/1500 [============================>.] - ETA: 0s - loss: 0.3354 - accuracy: 0.9071\n",
      "Epoch 92: val_loss improved from 0.31833 to 0.31786, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 762us/step - loss: 0.3350 - accuracy: 0.9070 - val_loss: 0.3179 - val_accuracy: 0.9131\n",
      "Epoch 93/100\n",
      "1452/1500 [============================>.] - ETA: 0s - loss: 0.3340 - accuracy: 0.9073\n",
      "Epoch 93: val_loss improved from 0.31786 to 0.31743, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 733us/step - loss: 0.3345 - accuracy: 0.9071 - val_loss: 0.3174 - val_accuracy: 0.9130\n",
      "Epoch 94/100\n",
      "1423/1500 [===========================>..] - ETA: 0s - loss: 0.3338 - accuracy: 0.9075\n",
      "Epoch 94: val_loss improved from 0.31743 to 0.31702, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 754us/step - loss: 0.3340 - accuracy: 0.9074 - val_loss: 0.3170 - val_accuracy: 0.9128\n",
      "Epoch 95/100\n",
      "1444/1500 [===========================>..] - ETA: 0s - loss: 0.3339 - accuracy: 0.9071\n",
      "Epoch 95: val_loss improved from 0.31702 to 0.31660, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 745us/step - loss: 0.3334 - accuracy: 0.9072 - val_loss: 0.3166 - val_accuracy: 0.9130\n",
      "Epoch 96/100\n",
      "1463/1500 [============================>.] - ETA: 0s - loss: 0.3327 - accuracy: 0.9075\n",
      "Epoch 96: val_loss improved from 0.31660 to 0.31616, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 768us/step - loss: 0.3329 - accuracy: 0.9074 - val_loss: 0.3162 - val_accuracy: 0.9132\n",
      "Epoch 97/100\n",
      "1424/1500 [===========================>..] - ETA: 0s - loss: 0.3325 - accuracy: 0.9075\n",
      "Epoch 97: val_loss improved from 0.31616 to 0.31577, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 740us/step - loss: 0.3324 - accuracy: 0.9075 - val_loss: 0.3158 - val_accuracy: 0.9133\n",
      "Epoch 98/100\n",
      "1447/1500 [===========================>..] - ETA: 0s - loss: 0.3323 - accuracy: 0.9082\n",
      "Epoch 98: val_loss improved from 0.31577 to 0.31537, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 767us/step - loss: 0.3319 - accuracy: 0.9080 - val_loss: 0.3154 - val_accuracy: 0.9131\n",
      "Epoch 99/100\n",
      "1480/1500 [============================>.] - ETA: 0s - loss: 0.3318 - accuracy: 0.9076\n",
      "Epoch 99: val_loss improved from 0.31537 to 0.31496, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 768us/step - loss: 0.3315 - accuracy: 0.9078 - val_loss: 0.3150 - val_accuracy: 0.9136\n",
      "Epoch 100/100\n",
      "1419/1500 [===========================>..] - ETA: 0s - loss: 0.3303 - accuracy: 0.9080\n",
      "Epoch 100: val_loss improved from 0.31496 to 0.31457, saving model to ./model_optimization\\mnist_MLP.hdf5\n",
      "1500/1500 [==============================] - 1s 791us/step - loss: 0.3310 - accuracy: 0.9079 - val_loss: 0.3146 - val_accuracy: 0.9137\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, verbose=1, validation_split=0.2, callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 571us/step - loss: 0.3146 - accuracy: 0.9124\n",
      "accuracy : 0.91\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test)\n",
    "print(f\"accuracy : {round(score[1],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_val_loss = history.history[\"val_loss\"]\n",
    "y_loss = history.history['loss']\n",
    "\n",
    "x_len = np.arange(len(y_loss))\n",
    "\n",
    "plt.plot(x_len, y_val_loss, marker='.', c='red', label='Testset_loss')\n",
    "plt.plot(x_len, y_loss, marker='.', c='blue', label='Trainset_loss')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fb4569285eef3a3450cb62085a5b1e0da4bce0af555edc33dcf29baf3acc1368"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
